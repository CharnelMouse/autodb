---
title: "Using autodb"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using autodb}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk:
    collapse: true
    comment: '#>'
    fig-width: 7
    fig-height: 5
---

```{r setup}
library(autodb)
```

# Terminology

To avoid confusion, this vignette is consistent with terminology: a *data frame* (table) has *records* (rows) with the same *attributes* (columns/variables), and we split it up into a *database*, that consists of *relations* (data frames with key and foreign key constraints between them). This is in line with terms used in the relational model.

When we talk about the type of values an attribute can take, we talk about a data *class*, as in the relational model and R, rather than a value type, as in many other programming languages.

# Motivation

`autodb` is described in the DESCRIPTION file in the following way:

> Automatic normalisation of a data frame to third normal form, with the intention of easing the process of data cleaning. (Usage to design your actual database for you is not advised.)

Let's cover that with some more detail.

## For data cleaning

Database normalisation works on the principle that our data should only express a piece of information in a single place. This is done to reduce the chance that a modification will accidentally change or remove other pieces of information.

Data cleaning works on the principle that we remove duplicate observations and structural errors, and have a clear understanding of any associations between different observations, or parts of observations. This is done to ensure a high quality for the data used in analysis, but I have found that it is equally important for making sure that I understand the domain I am hoping to model.

Normalisation is helpful for both of these areas. I usually use the original, flat, "tidy" data for the analysis itself. However, for checking the data's consistency, and for using the data to learn about the domain, normalised data is often helpful, whether it's used for the analysis or not.

I've often started data checking under the following conditions:

- One-off analyses, where the data is discard at the end, so it's not worth the time to get the data into a proper database.
- Limited, or unreliable, documentation. This makes it even more important to do data checking, not just for looking for errors, but also as an exercise in understanding the problem domain before modelling it.
- Data that fits in local memory, presenting less problems for automation.

It's in this context that I would usually have to do the normalisation by hand, and I would like to have a tool that does it semi-automatically. This package is intended to be that tool, using a minimal amount of additional libraries.

## Not for database design

Good database design is a hard and involved process, which requires a strong grasp on the domain that the database will hold information on. Please don't try to use this package to do it on autopilot. In particular, the conditions described above mean that the package does things differently to common practice:

- Since the package can only work with the data it's got, the resulting database schema is specific to the given dataset, rather than data of that type in general.
- As a special case of the above, we need to handle constant attributes, that have the same value in every record. In a real database, we would either not store them in the database at all, or place them with the expectation that they won't stay constant when more data is added. The former would hide information from the user, and the latter approach can't be decided automatically, so they're placed in a "constants" relation with an empty key. Empty keys are allowed in the relational model, but engines like SQL don't allow it in practice.
- It doesn't introduce artificial keys, or single-attribute lookup relations, which are a relational database's equivalent to factor levels.
- It doesn't create "virtual relations", also known as "views", which are data-less relations defined in terms of manipulating real relations (usually to create aggregates). This might change in the future.

## To third normal form

`autodb` gets a given data frame to third normal form: every attribute depends on the whole key(s), and non-key attributes depend on nothing but the key(s). This was chosen because there is an existing algorithm, Bernstein's synthesis, for normalising to third normal form, and because it's the highest normal form that is always attainable with only real relations and foreign key constraints.

An additional enhancement, called LTK form, is available as an option: see the section on avoidable attributes for more details.

# Simple examples

For most of these simple examples, we use the `ChickWeight` dataset, included with base R.

```{r}
summary(ChickWeight)
```

## Normalisation with `autodb`

The simplest way to do normalisation is with the `autodb` function:

```{r}
db <- autodb(ChickWeight)
db
```

## Plots

There is no plotting method within the `autodb` package itself. Instead, there are functions to write normalised objects as inputs to the Graphviz visualisation software, using the DOT language.

The generic function to do this is `gv`:

```{r}
db_text <- gv(db)
cat(db_text)
```

The resulting code can be saved to a file for passing to Graphviz elsewhere, or can be passed into an R function that renders Graphviz code. One option for the latter is `grViz` from the `DiagrammeR` package, but the result is an HTML widget: if you're writing a document, this requires inserting 2MB of Javascript code into the file. If you're writing a document in Quarto, you can call GraphViz directly using the `dot` engine, which avoids this.

```{r check_diagrammer}
if (requireNamespace("DiagrammeR", quietly = TRUE)) {
  show <- function(x) DiagrammeR::grViz(gv(x), width = "100%")
  maybe_plot <- function(x) DiagrammeR::grViz(gv(x), width = "100%")
}else{
  show <- print
  maybe_plot <- function(x) invisible(NULL)
}
```

```{r db_plot}
maybe_plot(db)
```

Each relation is represented as a box: the top row gives the relation name and the number of records, and the other rows list the attributes and their classes.

In the middle is a grid containing black cells: each column of black cells represents a *key* for the relation. Keys are irreducible subsets of a relation's attributes, that take a unique set of values in each record. They can be treated as a unique row identifier. For example, each chick has a unique `Chick` value, and each measurement has a unique `Time`-`Chick` pair.

In summary, `ChickWeight` is split into two relations, where the time and chick uniquely define a measurement -- or a row in the original data -- and the diet is determined by chick alone, i.e. chicks never have their diet changed.

`gv` also has methods for `data.frame`, and for `database_schema`, which is introduced later.

## Finding functional dependencies

Having looked at the final result first, we now look at the individual steps. The first step is to find the (non-trivial and minimal) dependencies present in the given data frame. There are various ways to do this; by default, the package uses FDHitsSep, a depth-first search algorithm. We run this using the `discover` function, setting `progress` to `TRUE` to see the steps taken:

```{r}
deps <- discover(ChickWeight, progress = TRUE)
deps
```

After simplifying the data to something quicker to iterate over -- specifically, by converting all attributes to have integer values -- the algorithm takes each attribute in turn as a possible dependant, and finds sets of the other attributes that determine it.

The result is a list of functional dependencies, in the format `determinant set -> dependant`, with an attribute named `attrs_order` that gives all the attribute names in their original order. Each of these three parts can be extracted:

```{r}
detset(deps)
dependant(deps)
attrs_order(deps)
```

The former two are useful for filtering, as we'll see later.

## Normalisation

Now that we have a list of discovered dependencies, we can construct a database schema, where the relation schemas are normalised to third normal form. This is done using a version of Bernstein's synthesis.

```{r}
schema <- synthesise(deps)
schema
```

Like the database before, we can also plot this database schema:

```{r}
maybe_plot(schema)
```

This is similar to the database plot given before, but there is some information not present, that requires the data frame itself. We do have class information about the attributes, extracted from the data frame during dependency discovery, but we have no record counts for the individual relation schemas. However, we do have automatically-generated names for the individual relations.

Additionally, at this point we have no connections between the relation schemas, since Bernstein's synthesis doesn't supply information about foreign key references. We could use this database schema to build a database, but we'd rather add the foreign key references first.

## Tuning detection and normalisation

Let's look at a different dataset for a moment, to look at some cases where we don't want to use the dependencies as given. We'll use the Titanic data set, also provided with base R. This data is in array form, so we first convert it to data frame form:

```{r}
knitr::kable(as.data.frame(Titanic))
```

This is a simple set of data, with a single count observation, `Freq`, for each combination of the four determining attributes. In other words, the relation is already normalised, so we only expect one relation in the normalised database.

If we use `autodb` again, we get the following database:

```{r}
show(autodb(as.data.frame(Titanic)))
```

Oops! The search found some functional dependencies where the count could be used to determine another attribute. These are clearly spurious: frequency count can't causally determine age, for example. However, the algorithm still finds these dependencies, because the counts are unique often enough to make these dependencies hold in the given data.

There are two approaches we can take to eliminate these spurious dependencies: not letting them be detected in the first place, and removing them before using `synthesise`.

To stop them being detected, we can put constraints on what is discovered by `discover`: we can ask for certain attributes to not be considered as determinants, or we can exclude attributes that inherit from certain classes. In this example, we could exclude `Freq` from being considered:

```{r}
titanic_deps_freqonly <- discover(as.data.frame(Titanic), exclude = "Freq")
titanic_deps_freqonly
```

Alternatively, we could exclude all attributes that inherit from "numeric":

```{r}
identical(titanic_deps_freqonly, discover(as.data.frame(Titanic), exclude_class = "numeric"))
```

These can both be used as arguments to `autodb` too:

```{r}
show(autodb(as.data.frame(Titanic), exclude = "Freq"))
```

Generally, excluding numeric attributes as determinants is often useful, because we expect non-integer numbers to be a measurement, not part of a primary key.

Alternatively, we could remove unwanted dependencies before using `decompose`. Here are the found dependencies, if we don't exclude anything:

```{r}
titanic_deps <- discover(as.data.frame(Titanic))
titanic_deps
```

We can remove the unwanted dependencies, where `Age` is the dependant:

```{r}
titanic_deps[dependant(titanic_deps) == "Freq"]
```

## Adding foreign key references

Getting back to our `ChickWeight` example, we now have a database schema, consisting of a list of `relation` schemas. However, we have no information about how these relation schemas are linked to each other. In particular, we have no information about foreign keys. We can add this information using `autoref`:

```{r}
linked_schema <- autoref(schema)
linked_schema
```

We could also have used `normalise`, instead of `synthesise` and `autoref` separately:

```{r}
normalise(deps)
```

Plotting this updated database schema shows the same relation schemas as before, linked together by foreign key references:

```{r}
show(linked_schema)
```

## Decomposing the original relation

Finally, once we have our normalised database schema, we can apply it to our original data frame, or a new one with the same structure. This results in a normalised database, as we got from using `autodb`:

```{r chickWeight_db2_plot}
db2 <- decompose(ChickWeight, linked_schema)
show(db2)
```

We now have the record counts added.

## Rejoining a database back into a data frame

We can reverse the process of turning a data frame into a database with the `rejoin` function. This may not be identical to `ChickWeight`, since the rows may have been rearranged. However, we can use the convenience function `df_equiv` to check for equivalence under row reordering:

```{r chickWeights_rejoin}
rejoined <- rejoin(db)
summary(rejoined)
identical(rejoined, ChickWeight)
df_equiv(rejoined, ChickWeight)
```

When rejoined, the relation attributes will be in the original order. However, the record order might have changed.

# Other features

## Approximate dependencies and database reduction

Larger datasets can often have entry errors, without an easy way to remove or deal with them. For this reason, we might be interested in "approximate" functional dependencies, which hold after removing a bounded amount of violating records.

Suppose that we normalise `nudge` again, without any manual dependency removal, but allowing approximate dependencies. We could cheat, knowing that the questionable data example found above showed there to be two questionable records: one for a duplicated publication ID, and one for a duplicated reference. Since `nudge` has `r nrow(nudge)` records, we can get rid of the resulting questionable dependencies by setting the accuracy to allow two discrepant records.

The `accuracy` argument for `discover` expects a number between zero and one, determining what proportion of a data frame's records need to satisfy a given dependency for the algorithm to consider it valid. By default, it's equal to one, so only exact dependencies are returned. If we set it to less than one, we need to use the slower DFD search algorithm, since approximate dependency search is not implemented for the default FDHitsSep algorithm.

```{r nudge_approximate_cheat}
show(normalise(discover(
  nudge,
  accuracy = 1 - 2/nrow(nudge),
  method = "DFD",
  exclude = c("n_study", "n_comparison", "n_control", "n_intervention"),
  exclude_class = "numeric"
)))
```

Currently `decompose` doesn't account for approximate dependencies, resulting in invalid databases, so here we just work with the database schema.

Compare this to the database schema we arrived at manually:

- The publication ID, the title, and the reference are considered equivalent, as we'd have originally expected.
- There is an additional relation, showing `approximation` to be approximately determined by some publication- and study-level attributes, rather than functionally determined at the effect-size level.

Lowering the accuracy results in more dependencies being found, and so in more relations. The number of relations can get very large. For example, suppose we instead set the accuracy to `0.99`, returning any approximate dependencies that hold in at least `r ceiling(nrow(nudge)*0.99)` records.

```{r nudge_approximate}
nudge_approx_database_schema <- discover(
  nudge,
  accuracy = 0.99,
  method = "DFD",
  exclude = c("n_study", "n_comparison", "n_control", "n_intervention"),
  exclude_class = "numeric"
) |>
  normalise()
show(nudge_approx_database_schema)
```

This is a little overwhelming, so we'll use a utility function called `reduce`. This returns only the named "main" relation, and any relations for which it is a descendant via foreign key references. Reducing the approximate schema, with the effect size table as the main table, gives us this set of relations:

```{r nudge_approximate_reduced}
show(reduce(nudge_approx_database_schema, "es_id"))
```

Questionable intermediate relations aside, we can see that there is now a publication-level relation with the publication ID as a key, since it determines the publication attributes once we remove one of the duplicate records we discovered before.

`reduce` can also be used on databases, where it considers each relation with the largest number of records as a main table. It needs to be used with caution: while the relations it removes are often spurious, it's possible to find databases where relations not linked to the main relation by foreign key references are required to rejoin to the original data frame, so reduction can remove necessary relations. Its intent is mostly to make glancing at database plots more manageable.

## Avoidable attributes

The next normal form after third normal form (3NF) is Boyes-Codd normal form (BCNF). Ensuring BCNF is enforced by the database is trickier, as in some cases it can't be enforced with just relations and foreign key constraints.

However, the package includes an option to convert to enhanced third normal form, also known as LTK form, which can be so enforced. This enhancement is tangential to BCNF, and could also be used to enhance schemas in BCNF.

In brief, the standard normal forms only put constraints on the attributes present in the relations one relation at a time. The enhancement is a constraint on the attributes present in a relation, while considering their presence in other relations. If a attribute in a relation can be removed, and still be determined from that relation by joining it to others, then the attribute is "avoidable", and can be removed. If the attribute is in any of the relation's keys, they'll be replaced by keys that use the attributes not being removed. This removes attributes from relations without removing any information from the database as a whole.

For example, we can take this simple example from Chapter 6 of The Theory of Relational Databases, by David Maier:

```{r avoid_setup}
avoid_deps <- functional_dependency(
  list(
    list("A", "B"),
    list("B", "A"),
    list(c("A", "C"), "D"),
    list(c("A", "C"), "E"),
    list(c("B", "D"), "C")
  ),
  attrs_order = c("A", "B", "C", "D", "E")
)
avoid_deps
avoid_schema <- normalise(avoid_deps)
show(avoid_schema)
```

Attributes `A` and `B` are equivalent, since relation `A` has them both as a key. In other words, relation `A` is a simple lookup relation. Because of this, we could remove `B` from relation `A_C`, and replace the key `B, D` with `A, D`, which is equivalent when accounting for relation `A`.

We can have this removal of avoidable attributes done automatically, using the `remove_avoidable` flag for `normalise`:

```{r avoid_remove}
avoid_schema_removed <- normalise(
  avoid_deps,
  remove_avoidable = TRUE
)
show(avoid_schema_removed)
```

This schema is now in LTK form, with no remaining avoidable attributes. We could have also removed `A` from relation `A_C` instead of `B`, so this process may not have a unique result. The package's implementation prefers to remove attributes that appear later in the original relation.
