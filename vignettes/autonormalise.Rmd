---
title: "Using autonormalise"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using autonormalise}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

```{r setup}
library(autonormalise)
```

# Motivation

`autonormalise` is described in the DESCRIPTION file in the following way:

> Automatic normalisation of a data.frame to third normal form, with the intention of easing the process of data cleaning."

Let us begin there.

## Data cleaning, not database design

Database normalisation works on the principle that our data should only express a piece of information in a single place. This is done to reduce the chance that a modification will accidentally change or remove other pieces of information.

Data cleaning works on the principle that we remove duplicate observations and structural errors, and have a clear understanding of any associations between different observations, or parts of observations. This is done to ensure a high quality for the data used in analysis, but I have found that it is equally important for making sure that I understand the domain I am hoping to model.

Normalisation is helpful for both of these areas. I usually use the original, flat, "tidy" data for the analysis itself. However, for checking the data's consistency, and for using the data to learn about the domain, normalised data is often helpful, whether it's used for the analysis or not.

Normalisation may, or may not, be a straightforward process, depending on how much information on the data you have. In my own work, I am lucky if I am given a key or a data dictionary for the data set at all. When I am, it usually proves to be wrong somehow. Normalisation, and data checking, is not just an exercise in looking for errors: it's also a key part of the process of understanding the problem domain. It's in this context that I would usually have to work out the normalisation by hand, and I would like to have a tool that does it semi-automatically. This package is intended to be that tool.

Good database design is a hard and involved process, which requires a strong grasp on the domain that the database will hold information on. Please don't try to use this package to do it on autopilot.

## Third normal form

`autonormalise` gets a given data.frame to third normal form: every attribute depends on the whole key(s), and non-key attributes depend on nothing but the key(s). This was chosen because there is an existing algorithm, Bernstein's synthesis, for normalising to third normal form, and because it's the highest normal form that is attainable with arbitrary original tables.

# A simple example

For a very simple example, we can use the `ChickWeight` dataset, included with base R.

```{r}
summary(ChickWeight)
```

## Normalisation with `autonorm`

The simplest way to do normalisation is with the `autonorm` function:

```{r}
db <- autonorm(ChickWeight, accuracy = 1, name = "Chick Weights")
```

The `accuracy` argument expects a number between zero and one, determining what proportion of a data.frames rows need to satisfy a given dependency for the algorithm to consider it valid. Setting `accuracy` to one, as done here, limits us to exact dependencies.

```{r}
db
```

## Plots

There is no plotting method within the `autonormalise` package itself. Instead, there are functions to write normalised objects as inputs to the Graphviz visualisation software, using the DOT language.

The generic function to do this is `gv`, which calls the `gv.database` method on our database, `db`:

```{r}
db_text <- gv(db)
cat(db_text)
```

This can be saved to a file for passing to Graphviz elsewhere, or can be passed into `grViz` in the `DiagrammeR` package:

```{r db_plot}
DiagrammeR::grViz(db_text)
```

Each table is represented as a box, with the top row giving the name and number of rows, and the other rows detailing the columns. In the middle is a grid of cells: each column of cells represents a (candidate) key for the table, where columns in that key have their cell filled in black.

We can see that `ChickWeight` was split into two tables, with a weight for each unique combination of Chick number and time, and the Chick number by itself determining the diet. We can also see the number of rows in each table, and the classes of the columns in those tables.

## Finding functional dependencies with DFD

Having looked at the final result first, we now look at the individual steps. The first step is to find the (non-trivial and minimal) dependencies present in the given data.frame. There are various ways to do this; this package uses DFD, a depth-first search algorithm. We run this using the `dfd` function, setting `progress` to `TRUE` to see the steps taken:

```{r}
deps <- dfd(ChickWeight, 1, progress = TRUE)
deps
```

After simplifying the data to something quicker to iterate over -- specifically, by converting all columns to integer values -- the algorithm takes each column in turn as a possible dependent, and finds sets of the other columns that determine it. As present, the algorithm can pass results for one column to future columns, to save computation time, but this is not implemented in the current version.

The result is a list, with two elements: `dependencies` gives the discovered determinants of each column in turn, and `attrs` gives all the column names.

This is meant to be a reasonably compact, user-readable presentation of the results, with dependencies grouped by the dependent. For use in the later stages of the process, we convert to a different format first, using `flatten`:

```{r}
flat_deps <- flatten(deps)
flat_deps
```

Dependencies are now separate entries, rather than being grouped by dependent.

## Normalisation

Now that we have a list of discovered dependencies, we can construct a database scheme, where the relation schemes are normalised to third normal form. This is done using a version of Bernstein's synthesis.

```{r}
scheme <- normalise(flat_deps)
scheme
```

Like the database before, we can also plot this database scheme:

```{r}
scheme_text <- gv(scheme)
cat(scheme_text)
DiagrammeR::grViz(scheme_text)
```

This is similar to the database plot given before, but there is some information not present, that requires the data frame itself. Namely, we have no class information about the columns, and no row counts for the individual relation schemes. Additionally, at this point we have no connections between the relation schemes, since Bernstein's synthesis doesn't supply information about foreign key references. We could use this database scheme to build a database, but we'd rather have the foreign key references, to guide table joins.

## Tuning detection and normalisation

Let's look at a different dataset for a moment, to look at some cases where we don't want to use the dependencies as given. We'll use the Titanic data set, also provided with base R. This data is in array form, so we first convert it to data frame form:

```{r}
as.data.frame(Titanic)
```

This is a simple set of data, with a single count observation, `Freq`, for each combination of the four determining variables. In other words, the table is already normalised, so we only expect one table in the normalised database.

If we use `autonorm` again, we get the following database:

```{r}
DiagrammeR::grViz(gv(autonorm(as.data.frame(Titanic), 1)))
```

Oops! The DFD algorithm found some functional dependencies where the count could be used to determine another variable. These are clearly spurious: frequency count clearly can't causally determine age, for example. However, the algorithm still finds these dependencies, because the counts are unique often enough to make these dependencies hold in the given data.

There are two approaches we can take to eliminate these spurious dependencies: not letting them be detected in the first place, and removing them before using `cross_reference`.

To stop them being detected, we can put constraints on what is discovered by `dfd`: we can ask for certain columns to not be considered as determinants, or we can exclude columns that inherit from certain classes. For example, here we could exclude `Freq` from being considered:

```{r}
titanic_deps_freqonly <- dfd(as.data.frame(Titanic), 1, exclude = "Freq")
titanic_deps_freqonly
```

Alternatively, we could exclude all columns that inherit from "numeric":

```{r}
identical(titanic_deps_freqonly, dfd(as.data.frame(Titanic), 1, exclude_class = "numeric"))
```

These can both be used as arguments to `autonorm` too:

```{r}
DiagrammeR::grViz(gv(autonorm(as.data.frame(Titanic), 1, exclude = "Freq")))
```

Generally, excluding numeric columns as determinants is often useful, because we expect non-integer numbers to be a measurement, not part of a primary key.

Alternatively, we could remove unwanted dependencies before using `decompose`. Here are the found dependencies, if we don't exclude anything:

```{r}
titanic_deps <- dfd(as.data.frame(Titanic), 1)
titanic_deps
```

We could remove the unwanted dependencies, where `Age` is the dependent:

```{r}
titanic_deps_filtered <- titanic_deps
titanic_deps_filtered$dependencies$Age <- list()
identical(titanic_deps_filtered, titanic_deps_freqonly)
```

Alternatively, we could look at the flattened version:

```{r}
flatten(titanic_deps)
```

We can remove unwanted dependencies from the flattened version, if it's more convenient:

```{r}
titanic_deps_filtered2 <- flatten(titanic_deps)
titanic_deps_filtered2$dependencies <- Filter(
  function(dep) dep[[2]] == "Freq", 
  titanic_deps_filtered2$dependencies
)
identical(flatten(titanic_deps_freqonly), titanic_deps_filtered2)
```

## Adding cross-references

Getting back to our `ChickWeight` example, we now have a database scheme, consisting of a list of relation schemes. However, we have no information about how these relation schemes are linked to each other. In particular, we have no information about foreign keys, that we would need to join tables back together, to reconstruct the original data frame. We can add this information using `cross_reference`:

```{r}
linked_scheme <- cross_reference(scheme)
linked_scheme
```

Plotting this updated database scheme shows the same relation schemes as before, linked together by foreign key references:

```{r}
linked_text <- gv(linked_scheme)
cat(linked_text)
DiagrammeR::grViz(linked_text)
```

## Decomposing the original table

Finally, once we have our normalised database scheme, we can apply it to our original data frame, or a new one with the same structure. This results in a normalised database, as we got from using `autonorm`:

```{r}
db2 <- decompose(ChickWeight, linked_scheme)
identical(db, db2)
```

```{r chickWeights_db_plot}
DiagrammeR::grViz(gv(db, name = "Chick Weights"))
```

We now have the column class information and row counts added, as well as automatic names for the individual relations.

## Approximate dependencies

## Avoidable attributes

# A larger example

Included in the package is a `r paste(dim(nudge), collapse = "-by-")` data frame called `nudge`:

```{r nudge_classes}
knitr::kable(
  data.frame(
    attribute = names(nudge),
    class = vapply(nudge, class, character(1))
  ),
  row.names = FALSE
)
```

This is the data set for a meta-analysis, looking at the effectiveness of "nudge" interventions. Measurements are taken within a three-layer hierarchy: publications contain studies, which contain effect size measurements. We expect this hierarchy to be apparent in the normalisation.

Getting full dependency information for this table can take a long time, so here we find a reduced set, not considering numeric or sample size columns:

```{r nudge_database}
nudge_deps <- flatten(dfd(
  nudge,
  accuracy = 1,
  exclude = c("n_study", "n_comparison", "n_control", "n_intervention"),
  exclude_class = "numeric"
))
nudge_scheme <- cross_reference(normalise(nudge_deps, remove_avoidable = TRUE))
DiagrammeR::grViz(gv(nudge_scheme))
```

We can see a table, with many columns, determined by the effect size ID number, `es_id`. This contains all of the numeric measurements, as expected in the table for the lowest level in the hierarchy. As also expected, this has a foreign key reference to a table for study-level information. However, the table this has a reference to is not determined by the publication ID, as we would expect. Instead, it is determined by the publication's title: to use the ID, we would need to supplement it with the publication year. This suggests that some publication ID numbers have been erroneously assigned to several publications, which we can easily test:

```{r nudge_publication_check}
nudge_database <- decompose(nudge, nudge_scheme, "nudge")
nudge_title_table <- nudge_database$tables$title$df
nudge_pid_duplicates <- unique(nudge_title_table$publication_id[
  duplicated(nudge_title_table$publication_id)
])
knitr::kable(subset(nudge_title_table, publication_id %in% nudge_pid_duplicates))
```

This is likely to also be why we have what appear to be tables for spurious functional dependencies, which have `publication_id` in a key.

Since the publication ID, therefore, can't be trusted, we could consider removing the column. Here, we'll remove all function dependencies where `publication_id` is in the determinant set, to get the same effect without re-running DFD:

```{r nudge_filter}
nudge_deps_filtered <- nudge_deps
nudge_deps_filtered$dependencies <- Filter(
  function(fd) !is.element("publication_id", fd[[1]]),
  nudge_deps_filtered$dependencies
)
nudge_scheme_filtered <- cross_reference(normalise(nudge_deps_filtered, remove_avoidable = TRUE))
DiagrammeR::grViz(gv(nudge_scheme_filtered))
```

This looks to be an improvement. However, we can still see some spurious tables involving `reference`. One of these combines it with `type_experiment` to determine `location`, a study-level variable. The other combine it with another variable to determine `title`. This is expected, since, in the publication-level table, the title determines the reference, but not vice versa. This means that there are publications that share a reference:

```{r nudge_reference_check}
nudge_database_filtered <- decompose(nudge, nudge_scheme_filtered, "nudge")
nudge_title_table_filtered <- nudge_database_filtered$tables$title$df
nudge_reference_duplicates <- unique(nudge_title_table_filtered$reference[
  duplicated(nudge_title_table_filtered$reference)
])
knitr::kable(subset(nudge_title_table_filtered, reference %in% nudge_reference_duplicates))
```

BETA is the Behavioural Economics Team of the Australian Government, so it's not surprising that they'd have multiple publications/reports per year. Giving them both the same reference would be awkward if those references were to be used, however.

While the reference is useful, the tables mentioned above are not, so we'll also remove functional dependencies with the reference determining something apart from the year:

```{r nudge_filter2}
nudge_deps_filtered2 <- nudge_deps_filtered
nudge_deps_filtered2$dependencies <- Filter(
  function(fd) !is.element("reference", fd[[1]]) || fd[[2]] == "year",
  nudge_deps_filtered2$dependencies
)
nudge_scheme_filtered2 <- cross_reference(normalise(nudge_deps_filtered2, remove_avoidable = TRUE))
DiagrammeR::grViz(gv(nudge_scheme_filtered2))
```

We now have `location` in a non-spurious table. Interestingly, removing dependencies has revealed extra information about study locations: studies of the same experiment type in a publication always have the same location. Looking at the resulting database shows that this removes many rows of what would be redundant location information if kept in the study table:

```{r nudge_clean_database}
nudge_database_filtered2 <- decompose(nudge, nudge_scheme_filtered2, "nudge")
DiagrammeR::grViz(gv(nudge_database_filtered2))
```

While this is not a dependency we could expect to hold if more data was collected, it's a reasonable dependency for the given data set.

# Planned extensions

## Class information in schemes

As shown in the plots above, database schemes don't contain information about the attribute classes: this is only added on database creation. This would potentially be useful if the original data.frame is large, and creating the database would take a long time.

## Better consistency checks

## Missing values

In relational databases, we commonly want to avoid the presence of missing values. For example, take the following table:

```{r example_table_with_NAs}
df_nas <- data.frame(
  patient_id = c(1L, 2L, 3L, 4L),
  trial_entry_date = as.Date(c("2022/05/02", "2022/06/06", "2022/04/01", "2022/03/19")),
  death_date = as.Date(c(NA, NA, "2022/10/07", NA))
)
df_nas
```

However, currently autonormalise treats `NA` as just another value, with no special treatment for normalisation or rejoining, resulting in no splitting of the table:

```{r example_table_with_NAs_autonorm}
DiagrammeR::grViz(gv(autonorm(df_nas, 1)))
```

In this case, a missing death date represents no death, and we would prefer to move death information to a separate table, containing only patients with a death date, in this case patient 3:

```{r example_table_with_NAs_nullably_normalised}
ideal_db <- structure(
  list(
    name = "patients",
    tables = list(
      patient = list(
        df = data.frame(
          patient_id = c(1L, 2L, 3L, 4L),
          trial_entry_date = as.Date(c("2022/05/02", "2022/06/06", "2022/04/01", "2022/03/19"))
        ),
        keys = list("patient_id"),
        index = "patient_id",
        parents = "death"
      ),
      death = list(
        df = data.frame(
          patient_id = 3L,
          death_date = as.Date("2022/10/07")
        ),
        keys = list("patient_id"),
        index = "patient_id",
        parents = character()
      )
    ),
    relationships = list(c("patient", "patient_id", "death", "patient_id")),
    attributes = c("patient_id", "trial_entry_date", "death_date")
  ),
  class = c("database", "list")
)
DiagrammeR::grViz(gv(ideal_db))
```

Handling cases like this requires automatically detecting, and normalising, nullable functional dependencies. This is planned for a later version, but is currently not available.
